---
layout: exp
title: 42Maru
accent_image: assets/img/42maru_bg.jpg
date_start: 2018-09-10
date_end: now
description: Machine Learning Engineer
---

I worked on several NLP tasks as a Machine Learning Engineer in 42Maru, such as English paraphrasing, sentence similarity, machine reading comprehension, and I am now working on text summarization.

---

* this unordered seed list will be replaced by toc as unordered list
{:toc}

## Company

[42Maru](https://www.42maru.ai/) is a korean start-up, based in Seoul. They run QA (Question Answering) solutions and always aim higher and higher. 

## Projects

### English Paraphrasing

The goal of this first project was to create a neural network for English paraphrasing.

Previously, different architectures were used by the company for Korean paraphrasing. 

Skills & technologies
{:.lead}

I've worked with 2 datasets, the [Quora question pairs](https://www.kaggle.com/quora/question-pairs-dataset) dataset, and the [SNLI](https://nlp.stanford.edu/projects/snli/) dataset.

I worked with `Keras` and implemented a [Pervasive Attention](https://arxiv.org/abs/1808.03867) neural network. 

### Document Similarity

For this project, I used knowledge acquired in previous project in order to come with a sentence similarity architecture for English dataset. 

Like previous project, I worked on this task alone.

Skills & technologies
{:.lead}

Since this project started right after [BERT](https://github.com/google-research/bert) came out, I was excited to use this awesome model for my task.

To solve the problem of lengthy documents, I came up with a Siamese network, implemented with `Keras`, on top of BERT. I've used [BERT as service](https://github.com/hanxiao/bert-as-service) to make the process easier.

Despite the weirdness of the network, I could reach great results. One idea I came up with actually helped and was used by another member of the company, which increased the top score of the architecture for the SQuAD 2.0 leaderboard.

There is no Document Similarity dataset in English, so I used [SICK](http://clic.cimec.unitn.it/composes/sick.html) dataset and [STS-B](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) dataset. 

### Machine Reading Comprehension

Previously, another member of the company wrote a neural network architecture for machine reading comprehension and reached a top rank in the SQuAD 2.0 leaderboard :

![](/assets/img/experience/squad.png)

For this project, I worked together with 3 other members of 42Maru. As a first step, we tried to increase the performance of the neural network. Then we implemented 2 API (English + Korean) to make machine reading comprehension available through a website :

![](/assets/img/experience/mrc.gif)

Skills & technologies
{:.lead} 

For this project, we used `Tensorflow`, as well as `Flask` for the API, and of course [BERT](https://github.com/google-research/bert).

The model was trained on TPU through [Google Colab](https://colab.research.google.com/).

We used the [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/) dataset.

### Text Summarization

This project involved me and 2 others members of 42Maru. After reading several papers on the subject, we decided to improve a model mixing Abstractive and Extractive summarization.

Skills & technologies
{:.lead}

We decided to use `Pytorch` because of the paper we chose : [Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting](https://arxiv.org/abs/1805.11080).

This paper used several networks, such as **Pointer network**, **LSTM with Attention**, etc...

It was a great opportunity for me to discover `Pytorch` and **Reinforcement Learning**.

## What I learned

Being the first foreigner of the company is tough ! But at the same time, integration was so easy, because I was surrounded by amazing people who made plenty of effort towards me.

I learned so much from the great people of this company. They have great technical knowledge in their field, and they truly inspired me.

I undoubtedly enjoyed working here, and can't imagine a better job right after graduation. Every single project that I worked on taught me a lot, and I loved every single one of these projects. 