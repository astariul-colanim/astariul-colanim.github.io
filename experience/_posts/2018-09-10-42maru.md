---
layout: exp
title: 42Maru
accent_image: assets/img/42maru_bg.jpg
date_start: 2018-09-10
date_end: now
description: Machine Learning Engineer
---

I worked on several NLP tasks as a Machine Learning Engineer in 42Maru, such as English paraphrasing, sentence similarity, machine reading comprehension, and I am now working on text summarization.

---

* this unordered seed list will be replaced by toc as unordered list
{:toc}

## Company

[42Maru](https://www.42maru.ai/) is a korean start-up, based in Seoul. They run QA (Question Answering) solutions and always aim higher and higher. 

## Projects

### English Paraphrasing

The goal of this first project was to create a neural network for English paraphrasing.

Previously, different architectures were used by the company for Korean paraphrasing. 

Skills & technologies
{:.lead}

I've worked with 2 datasets, the [Quora question pairs](https://www.kaggle.com/quora/question-pairs-dataset) dataset, and the [SNLI](https://nlp.stanford.edu/projects/snli/) dataset.

I worked with `Keras` and implemented a [Pervasive Attention](https://arxiv.org/abs/1808.03867) neural network. 

### Document Similarity

For this project, I used knowledge acquired in previous project in order to come with a sentence similarity architecture for English dataset. 

Like previous project, I worked on this task alone.

Skills & technologies
{:.lead}

Since this project started right after [BERT](https://github.com/google-research/bert) came out, I was excited to use this awesome model for my task.

To solve the problem of lengthy documents, I came up with a Siamese network, implemented with `Keras`, on top of BERT. I've used [BERT as service](https://github.com/hanxiao/bert-as-service) to make the process easier.

Despite the weirdness of the network, I could reach great results. One idea I came up with actually helped and was used by another member of the company, which increased the top score of the architecture for the SQuAD 2.0 leaderboard.

There is no Document Similarity dataset in English, so I used [SICK](http://clic.cimec.unitn.it/composes/sick.html) dataset and [STS-B](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) dataset. 

### Machine Reading Comprehension

Previously, another member of the company wrote a neural network architecture for machine reading comprehension and reached a top rank in the SQuAD 2.0 leaderboard :

![](/assets/img/experience/squad.png)

For this project, I worked together with 3 other members of 42Maru. As a first step, we tried to increase the performance of the neural network. Then we implemented 2 API (English + Korean) to make machine reading comprehension available through a website :

![](/assets/img/experience/mrc.gif)

Skills & technologies
{:.lead} 

For this project, we used `Tensorflow`, as well as `Flask` for the API, and of course [BERT](https://github.com/google-research/bert).

The model was trained on TPU through [Google Colab](https://colab.research.google.com/).

We used the [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/) dataset.

### Question Generation

In order to improve the MRC model through data augmentation, a coworker and me looked into the Question Generation task. 

We implemented a basic attentional encoder-decoder architecture. We then tried to improve the performance of this architecture, and we succeed by using a modified BERT as encoder (and LSTM as decoder). Our score were actually state of the art on SQuAD dataset for Question Generation :

|        | Previous SOTA | Our Model |
|:------:|:-------------:|:---------:|
| BLEU 1 |     45.07     |   49.85   |
| BLEU 2 |     29.58     |   36.79   |
| BLEU 3 |     21.60     |   29.36   |
| BLEU 4 |     16.38     |   23.98   |

 Shortly after, I moved onto next project (Text Summarization), but my coworker kept working on this task and could push the score further (~0.3 point) using Adversarial Learning. 
 
Skills & technologies
{:.lead} 

For this task we decided to use `Pytorch`, because it was easier to work together on this framework.

I learned more about **BERT** as well as **attentional** models, and it was a great opportunity for me to discover a new framework, `Pytorch`.

### Text Summarization

This is the project I'm currently working on. I'm working on it alone.  
I have to implement the core component of a service for Text Summarization. 

I first used an Extractive + Abstractive approach, but after a few trials, I couldn't improve the score further.

When the code of [Presumm](https://github.com/nlpyang/PreSumm) was released, I decided to work from this as it was the current SOTA.  
By using XLNet, I could improve the current state of the art on CNN/DM dataset in both Extractive Summarization and Abstractive Summarization :

**Extractive summarization :**

|         | PreSumm (Previous SOTA) |  Ours  |
|:-------:|:-----------------------:|:------:|
| ROUGE 1 |        43.23            | 43.73  |
| ROUGE 2 |        20.24            | 20.50  |
| ROUGE L |        39.63            | 40.08  |

**Abstractive Summarization :**

|         | PreSumm (Previous SOTA) |  Ours  |
|:-------:|:-----------------------:|:------:|
| ROUGE 1 |        42.13            | 42.60  |
| ROUGE 2 |        19.60            | 20.16  |
| ROUGE L |        39.18            | 39.67  |

In addition to better scores, my contributions allowed to generate summary from documents of any length, while the original PreSumm code has a limitation on the input document. 

---

Later, Facebook researchers released [BART](https://github.com/pytorch/fairseq/tree/master/examples/bart), beating the SOTA by a large margin.  
So I studied the model, as well as the framework used : [Fairseq](https://github.com/pytorch/fairseq).

After several trials, I improved BART model by adding a mecanism allowing us to control the abstractiveness of generated summaries, without even re-training the model ! This mecanism also gave better results :

|         |   BART (Previous SOTA)  |  Pegasus (Current SOTA) |  Ours  |
|:-------:|:-----------------------:|:-----------------------:|:------:|
| ROUGE 1 |        44.16            |        44.17            | 44.86  |
| ROUGE 2 |        21.28            |        21.47            | 21.60  |
| ROUGE L |        40.90            |        41.11            | 41.77  |

## What I learned

Being the first foreigner of the company is tough ! But at the same time, integration was so easy, because I was surrounded by amazing people who made plenty of efforts toward me.

I learned so much from the great people of this company. They have great technical knowledge in their field, and they truly inspired me.

I undoubtedly enjoyed working here, and can't imagine a better job right after graduation. Every single project that I worked on taught me a lot, and I loved every single one of these projects. 
